# llm-query-pipeline

This public repository contains a project focused on generating SPARQL from Natural Language Queries, leveraging OpenAI's ChatGPT 4.0. The purpose of this initiative is to serve as a proof of concept demonstrating the combined use of competency questions and Large Language Models in evaluating knowledge graphs.

To accomplish this, two distinct use cases have been outlined:
(i) Using a public ontology known as [SAREF](https://saref.etsi.org/). The whole setup is published.
(ii) Applying the concept to a confidential ontology called OPC UA Robotics. For more information please download OPC UA Knowledge Extraction (OKE) dataset from [here](https://zenodo.org/records/10573256). In this repository, we have made the compliance rules and verified SPARQL Queries publicly available.

In this repository, a smaller version of the SAREF ontology and populated knowledge graph is shared. The code can be executed using the following command in the main directory.

The flow involves numerous sub-steps. Initially, we must create an LLM prompt for each question listed in the Excel file. Each question is dispatched as an individual API Call to ChatGPT. The prompt is constructed using the prompt template, ontology, and the question itself. The API can be invoked using the API_KEY and ENDPOINT, which should be customized with your unique key and endpoint. After substituting these two parameters for the LLM call, it can be executed.

```bash  
python main.py saref  
```

The generated_queries.xslx file i the output folder is created. It encompasses all competency questions, verified SPARQL Queries, expected outcomes, and the queries that have been automatically generated by our software. 

For the evaluation this can be run:
```bash  
python evaluate.py saref  
```

## Limitations
Currently, it works for SAREF ontology. If you wish to use our private Robotics ontology, you'll need to request permission from our team. You can also use it for other ontologies or your own custom ontology and knowledge graph, in conjunction with your competency questions.
All you need to do is create a folder with your ontology name (for example, "saref") and within that folder, you should place your ontology, knowledge graph, and questions with the same naming as in the example. You have the option to either generate your own template or use the default one. Once this setup is complete, you can run the main.py using your folder name.

```bash  
python main.py <your_folder_name>  
```

The second limitation pertains to the size of the ontology. As of now, ChatGPT can support a prompt size up to 8000 tokens.



## Authors and acknowledgment
Special thanks go to Siemens AG and TU Wien for their collaboration and contributions.

## License
For open source projects, say how it is licensed.

## Project status
If you have run out of energy or time for your project, put a note at the top of the README saying that development has slowed down or stopped completely. Someone may choose to fork your project or volunteer to step in as a maintainer or owner, allowing your project to keep going. You can also make an explicit request for maintainers.
